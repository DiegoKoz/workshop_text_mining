---
title: "Aplicación: Descarga de Tweets, limpieza y nube de palabras"
output: html_notebook
---


```{r message=FALSE, warning=FALSE}
# install.packages("rtweet")
library(rtweet)
library(tidyverse)
library(tm)
library(wordcloud2)
```


```{r}
rt <- search_tweets(q = "#NLP OR #datascience OR text mining OR #textmining OR #TextMining  OR NLP",type = "recent", n = 18000, include_rts = FALSE)
# rt <- search_tweets(q = "#QueSeaLey OR #AbortoLegal OR ##AbortoLegalYa",type = "recent", n = 18000, include_rts = FALSE)
```

```{r}
rt
```


```{r}
#saveRDS(rt,'rt.RDS')
#rt <- read_rds('rt.RDS')
```


```{r}
glimpse(rt)
```

```{r}
range(rt$created_at)
```

Nos da los tweets de los últimos nueve días.


```{r}
rt %>%
  ts_plot("3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frecuencia del hash #rstats De los ultimos 9 dias",
    subtitle = "Agregado a intervalos de tres horas")
```


```{r}
texto <-  rt$text 
```


```{r}
myCorpus = Corpus(VectorSource(texto))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords, stopwords(kind = "es"))
myDTM = TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
m = as.matrix(myDTM)
M <- sort(rowSums(m), decreasing = TRUE)
```


```{r}
myDTM
```


```{r}
dim(m)
```


```{r}
m[1:10,1:10]
```



```{r}
vv <- data_frame(word = names(M), freq = M)
vv <- vv %>% 
  filter(freq>=5) %>% 
  top_n(.,n=25, wt = freq)
    
wordcloud2(vv, shuffle = FALSE)
```


